{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "MXXTuy_o0sjk"
   },
   "outputs": [],
   "source": [
    "# !pip install -q -U numpy pandas kaggle_environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "yz23vWHD0wcj"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "from kaggle_environments import make, evaluate\n",
    "from kaggle_environments.envs.rps.utils import get_score\n",
    "\n",
    "def printl(text):\n",
    "    # Обёртка print для агентов\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0kuo6IOxiRub"
   },
   "source": [
    "## Опишем поведение нескольких агентов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "bqTqV7B92rJ6"
   },
   "outputs": [],
   "source": [
    "# Static agents\n",
    "def rock_agent(observation, configuration):\n",
    "    \"\"\"Агент, который всегда выбирает жест \"камень\".\n",
    "\n",
    "    Args:\n",
    "        observation (object): Наблюдение о состоянии раунда:\n",
    "            - observation.step (int): номер шага.\n",
    "            - observation.reward (int): величина награды.\n",
    "        configuration (object): Конфигурация игры:\n",
    "            - configuration.signs (int): количество возможных жестов.\n",
    "\n",
    "    Returns:\n",
    "        int: константа - 0.\n",
    "    \"\"\"\n",
    "    printl(\"Agent action - 0\")\n",
    "    return 0\n",
    "\n",
    "def paper_agent(observation, configuration):\n",
    "    \"\"\"Агент, который всегда выбирает жест \"бумага\".\n",
    "\n",
    "    Args:\n",
    "        observation (object): Наблюдение о состоянии раунда:\n",
    "            - observation.step (int): номер шага.\n",
    "            - observation.reward (int): величина награды.\n",
    "        configuration (object): Конфигурация игры:\n",
    "            - configuration.signs (int): количество возможных жестов.\n",
    "\n",
    "    Returns:\n",
    "        int: константа - 1.\n",
    "    \"\"\"\n",
    "    printl(\"Agent action - 1\")\n",
    "    return 1\n",
    "\n",
    "def scissors_agent(observation, configuration):\n",
    "    \"\"\"Агент, который всегда выбирает жест \"ножницы\".\n",
    "\n",
    "    Args:\n",
    "        observation (object): Наблюдение о состоянии раунда:\n",
    "            - observation.step (int): номер шага.\n",
    "            - observation.reward (int): величина награды.\n",
    "        configuration (object): Конфигурация игры:\n",
    "            - configuration.signs (int): количество возможных жестов.\n",
    "\n",
    "    Returns:\n",
    "        int: константа - 2.\n",
    "    \"\"\"\n",
    "    printl(\"Agent action - 2\")\n",
    "    return 2\n",
    "\n",
    "def random_agent(observation, configuration):\n",
    "    \"\"\"Агент, который всегда выбирает случайный жест.\n",
    "\n",
    "    Args:\n",
    "        observation (object): Наблюдение о состоянии раунда:\n",
    "            - observation.step (int): номер шага.\n",
    "            - observation.reward (int): величина награды.\n",
    "        configuration (object): Конфигурация игры:\n",
    "            - configuration.signs (int): количество возможных жестов.\n",
    "\n",
    "    Returns:\n",
    "        int: число из диапазона [0, configuration.signs - 1].\n",
    "    \"\"\"\n",
    "    move = random.randrange(0, configuration.signs)\n",
    "    printl(f\"Agent action {move}\")\n",
    "    return move\n",
    "\n",
    "# Based on lib\n",
    "def copy_opponent_agent(observation, configuration):\n",
    "    \"\"\"Агент, повторяющий последний жест противника.\n",
    "\n",
    "    Args:\n",
    "        observation (object): Наблюдение о состоянии раунда:\n",
    "            - observation.step (int): номер шага.\n",
    "            - observation.reward (int): величина награды.\n",
    "        configuration (object): Конфигурация игры:\n",
    "            - configuration.signs (int): количество возможных жестов.\n",
    "\n",
    "    Returns:\n",
    "        int: число из диапазона [0, configuration.signs - 1].\n",
    "    \"\"\"\n",
    "    if observation.step > 0:\n",
    "        move = observation.lastOpponentAction  \n",
    "    else:\n",
    "        move = random.randrange(0, configuration.signs)  \n",
    "    printl(f\"Agent action {move}\")\n",
    "    return move\n",
    "\n",
    "def respond_to_opponent_agent(observation, configuration):\n",
    "    \"\"\"Агент, выбирающий жест, который побеждает предыдущий жест противника.\n",
    "\n",
    "    Args:\n",
    "        observation (object): Наблюдение о состоянии раунда:\n",
    "            - observation.step (int): номер шага.\n",
    "            - observation.reward (int): величина награды.\n",
    "        configuration (object): Конфигурация игры:\n",
    "            - configuration.signs (int): количество возможных жестов.\n",
    "\n",
    "    Returns:\n",
    "        int: число из диапазона [0, configuration.signs - 1].\n",
    "    \"\"\"\n",
    "    if observation.step == 0:\n",
    "        move = random.randrange(0, configuration.signs)  \n",
    "    else:\n",
    "        move = ( 1 + observation.lastOpponentAction ) % 3\n",
    "    printl(f\"Agent action {move}\")  \n",
    "    return move\n",
    "\n",
    "agent_sequence_sign=0\n",
    "agent_sequence_order=[0, 1, 2]\n",
    "\n",
    "def agent_sequence(observation, configuration):\n",
    "    \"\"\"Агент, который выбирающий жесты в строгой последовательности.\n",
    "\n",
    "    Args:\n",
    "        observation (object): Наблюдение о состоянии раунда:\n",
    "            - observation.step (int): номер шага.\n",
    "            - observation.reward (int): величина награды.\n",
    "        configuration (object): Конфигурация игры:\n",
    "            - configuration.signs (int): количество возможных жестов.\n",
    "\n",
    "    Returns:\n",
    "        int: число из диапазона [0, configuration.signs - 1].\n",
    "    \"\"\"\n",
    "    global agent_sequence_sign\n",
    "    move = agent_sequence_order[agent_sequence_sign] \n",
    "    agent_sequence_sign = (agent_sequence_sign + 1) % len(agent_sequence_order)\n",
    "    printl(f\"Agent action {move}\")\n",
    "    return move\n",
    "\n",
    "common_selection_agent_stats={0: 0, 1: 0, 2: 0}\n",
    "def common_selection_agent(observation, configuration):\n",
    "    \"\"\"Агент, который выбирающий жесты, противодействующие наиболее частому ходу противника.\n",
    "\n",
    "    Args:\n",
    "        observation (object): Наблюдение о состоянии раунда:\n",
    "            - observation.step (int): номер шага.\n",
    "            - observation.reward (int): величина награды.\n",
    "        configuration (object): Конфигурация игры:\n",
    "            - configuration.signs (int): количество возможных жестов.\n",
    "\n",
    "    Returns:\n",
    "        int: число из диапазона [0, configuration.signs - 1].\n",
    "    \"\"\"\n",
    "    global common_selection_agent_stats\n",
    "    if observation.step > 0:\n",
    "        last_move = observation.lastOpponentAction\n",
    "        common_selection_agent_stats[last_move] += 1  \n",
    "    else:\n",
    "        return random.randrange(0, configuration.signs)  \n",
    "    most_common_move = max(common_selection_agent_stats, key=common_selection_agent_stats.get)\n",
    "    move = ( 1 + most_common_move ) % 3\n",
    "    printl(f\"Agent action {move}\")\n",
    "    return move\n",
    "\n",
    "reward_check_agent_sign = 0\n",
    "reward_check_agent_order = [2, 2, 2, 0, 1]\n",
    "reward_check_agent_def = random.sample(reward_check_agent_order, 3)\n",
    "def reward_check_agent(observation, configuration):\n",
    "    \"\"\"Агент, который принимает решения на основе полученной награды и\n",
    "    последнего хода противника.\n",
    "\n",
    "    Этот агент использует три различных стратегии в зависимости от награды:\n",
    "    - `reward <= 25` - жест, противодействующий ходу противника\n",
    "    - `reward > 25 && reward <= 75` - случайный жест.\n",
    "    - `reward > 75` - использование заранее определенного порядка\n",
    "\n",
    "    Args:\n",
    "        observation (object): Наблюдение о состоянии раунда:\n",
    "            - observation.step (int): номер шага.\n",
    "            - observation.reward (int): величина награды.\n",
    "        configuration (object): Конфигурация игры:\n",
    "            - configuration.signs (int): количество возможных жестов.\n",
    "\n",
    "    Returns:\n",
    "        int: число из диапазона [0, configuration.signs - 1].\n",
    "    \"\"\"\n",
    "    global reward_check_agent_sign\n",
    "    if observation.step == 0:\n",
    "        move = random.randrange(0, configuration.signs)\n",
    "    else:\n",
    "        if observation.reward <= 25:\n",
    "            opponent_move = observation.lastOpponentAction\n",
    "            if opponent_move == 0:  \n",
    "                move = 1 \n",
    "            elif opponent_move == 1: \n",
    "                move = 2 \n",
    "            elif opponent_move == 2:  \n",
    "                move = 0     \n",
    "        elif 25 < observation.reward <= 75:\n",
    "            move = random.randrange(0, configuration.signs)  \n",
    "        else:\n",
    "            move = reward_check_agent_def[reward_check_agent_sign]\n",
    "            reward_check_agent_sign += 1\n",
    "            if reward_check_agent_sign >= len(reward_check_agent_def):  \n",
    "                reward_check_agent_def = random.sample(reward_check_agent_order, 3)  \n",
    "                reward_check_agent_sign = 0\n",
    "    printl(f\"Agent action {move}\")\n",
    "    return move\n",
    "\n",
    "anti_replay_agent_last_move = None\n",
    "def anti_replay_agent(observation, configuration):\n",
    "    \"\"\"Агент, который избегает повторения одного и того же хода дважды подряд.\n",
    "\n",
    "    Args:\n",
    "        observation (object): Наблюдение о состоянии раунда:\n",
    "            - observation.step (int): номер шага.\n",
    "            - observation.reward (int): величина награды.\n",
    "        configuration (object): Конфигурация игры:\n",
    "            - configuration.signs (int): количество возможных жестов.\n",
    "\n",
    "    Returns:\n",
    "        int: число из диапазона [0, configuration.signs - 1].\n",
    "    \"\"\"\n",
    "    global anti_replay_agent_last_move\n",
    "    if observation.step == 0:\n",
    "        move = random.choice([0, 1, 2])\n",
    "    else:\n",
    "        new_order = [i for i in [0, 1, 2] if i != anti_replay_agent_last_move]\n",
    "        move = random.choice(new_order)  \n",
    "    anti_replay_agent_last_move = move\n",
    "    printl(f\"Agent action {move}\")\n",
    "    return move\n",
    "\n",
    "def predictive_agent(observation, configuration):\n",
    "    \"\"\"Агент-предсказатель, который выбирает ход на основе вероятности действий противника.\n",
    "\n",
    "    Args:\n",
    "        observation (object): Наблюдение о состоянии раунда:\n",
    "            - observation.step (int): номер шага.\n",
    "            - observation.reward (int): величина награды.\n",
    "        configuration (object): Конфигурация игры:\n",
    "            - configuration.signs (int): количество возможных жестов.\n",
    "\n",
    "    Returns:\n",
    "        int: число из диапазона [0, configuration.signs - 1].\n",
    "    \"\"\"\n",
    "    if observation.step == 0:\n",
    "        return random.randrange(0, configuration.signs)  \n",
    "        \n",
    "    opponent_move = observation.lastOpponentAction\n",
    "    best_move = (opponent_move + 1) % configuration.signs  \n",
    "\n",
    "    if random.random() < 0.8:  \n",
    "        move = best_move\n",
    "    else:\n",
    "        move = random.randrange(0, configuration.signs) \n",
    "    printl(f\"Agent action {move}\")\n",
    "    return move\n",
    "\n",
    "reactionary_agent_last_react_action = None\n",
    "def reactionary_agent(observation, configuration):\n",
    "    \"\"\"Агент, который реагирует на ход противника, выбирая жест, который противодействует его предыдущему ходу.\n",
    "\n",
    "    Args:\n",
    "        observation (object): Наблюдение о состоянии раунда:\n",
    "            - observation.step (int): номер шага.\n",
    "            - observation.reward (int): величина награды.\n",
    "        configuration (object): Конфигурация игры:\n",
    "            - configuration.signs (int): количество возможных жестов.\n",
    "\n",
    "    Returns:\n",
    "        int: число из диапазона [0, configuration.signs - 1].\n",
    "    \"\"\"\n",
    "    global reactionary_agent_last_react_action\n",
    "    if observation.step == 0:\n",
    "        reactionary_agent_last_react_action = random.randrange(0, configuration.signs)\n",
    "    elif get_score(reactionary_agent_last_react_action, observation.lastOpponentAction) <= 1:\n",
    "        reactionary_agent_last_react_action = (observation.lastOpponentAction + 1) % configuration.signs\n",
    "    move = reactionary_agent_last_react_action\n",
    "    printl(f\"Agent action {move}\")\n",
    "    return move\n",
    "\n",
    "\n",
    "counter_reactionary_agent_last_counter_action = None\n",
    "def counter_reactionary_agent(observation, configuration):\n",
    "    \"\"\"Агент, который реагирует на ход противника, используя стратегию контр-реакции.\n",
    "\n",
    "    Args:\n",
    "        observation (object): Наблюдение о состоянии раунда:\n",
    "            - observation.step (int): номер шага.\n",
    "            - observation.reward (int): величина награды.\n",
    "        configuration (object): Конфигурация игры:\n",
    "            - configuration.signs (int): количество возможных жестов.\n",
    "\n",
    "    Returns:\n",
    "        int: число из диапазона [0, configuration.signs - 1].\n",
    "    \"\"\"\n",
    "    global counter_reactionary_agent_last_counter_action\n",
    "    if observation.step == 0:\n",
    "        counter_reactionary_agent_last_counter_action = random.randrange(0, configuration.signs)\n",
    "    elif get_score(counter_reactionary_agent_last_counter_action, observation.lastOpponentAction) == 1:\n",
    "        counter_reactionary_agent_last_counter_action = (counter_reactionary_agent_last_counter_action + 2) % configuration.signs\n",
    "    else:\n",
    "        counter_reactionary_agent_last_counter_action = (observation.lastOpponentAction + 1) % configuration.signs\n",
    "    move = counter_reactionary_agent_last_counter_action\n",
    "    printl(f\"Agent action {move}\")\n",
    "    return move\n",
    "\n",
    "statistical_agent_action_histogram = {}\n",
    "def statistical_agent(observation, configuration):\n",
    "    \"\"\"Агент, который реагирует на ход противника, основываясь на статистике предыдущих ходов.\n",
    "\n",
    "    Args:\n",
    "        observation (object): Наблюдение о состоянии раунда:\n",
    "            - observation.step (int): номер шага.\n",
    "            - observation.reward (int): величина награды.\n",
    "        configuration (object): Конфигурация игры:\n",
    "            - configuration.signs (int): количество возможных жестов.\n",
    "\n",
    "    Returns:\n",
    "        int: число из диапазона [0, configuration.signs - 1].\n",
    "    \"\"\"\n",
    "    global statistical_agent_action_histogram\n",
    "    if observation.step == 0:\n",
    "        statistical_agent_action_histogram = {}\n",
    "        return\n",
    "    action = observation.lastOpponentAction\n",
    "    if action not in statistical_agent_action_histogram:\n",
    "        statistical_agent_action_histogram[action] = 0\n",
    "    statistical_agent_action_histogram[action] += 1\n",
    "    mode_action = None\n",
    "    mode_action_count = None\n",
    "    for k, v in statistical_agent_action_histogram.items():\n",
    "        if mode_action_count is None or v > mode_action_count:\n",
    "            mode_action = k\n",
    "            mode_action_count = v\n",
    "            continue\n",
    "    move = (mode_action + 1) % configuration.signs\n",
    "    printl(f\"Agent action {move}\")\n",
    "    return move"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Всего агентов: 14\n",
      "counter_reactionary_agent: 15 побед\n",
      "anti_replay_agent: 13 побед\n",
      "reactionary_agent: 12 побед\n",
      "respond_to_opponent_agent: 11 побед\n",
      "reward_check_agent: 10 побед\n",
      "predictive_agent: 10 побед\n",
      "statistical_agent: 8 побед\n",
      "copy_opponent_agent: 6 побед\n",
      "common_selection_agent: 5 побед\n",
      "scissors_agent: 3 побед\n",
      "rock_agent: 2 побед\n",
      "paper_agent: 2 побед\n",
      "random_agent: 2 побед\n",
      "agent_sequence: 2 побед\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Agent</th>\n",
       "      <th>Average Reward</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>counter_reactionary_agent</td>\n",
       "      <td>61.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>reactionary_agent</td>\n",
       "      <td>52.653846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>anti_replay_agent</td>\n",
       "      <td>38.307692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>predictive_agent</td>\n",
       "      <td>35.384615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>respond_to_opponent_agent</td>\n",
       "      <td>34.884615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>statistical_agent</td>\n",
       "      <td>16.038462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>agent_sequence</td>\n",
       "      <td>14.423077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>copy_opponent_agent</td>\n",
       "      <td>3.423077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>random_agent</td>\n",
       "      <td>-0.076923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>reward_check_agent</td>\n",
       "      <td>-15.115385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>common_selection_agent</td>\n",
       "      <td>-30.192308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>scissors_agent</td>\n",
       "      <td>-60.230769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>paper_agent</td>\n",
       "      <td>-67.923077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rock_agent</td>\n",
       "      <td>-82.576923</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Agent  Average Reward\n",
       "12  counter_reactionary_agent       61.000000\n",
       "11          reactionary_agent       52.653846\n",
       "9           anti_replay_agent       38.307692\n",
       "10           predictive_agent       35.384615\n",
       "5   respond_to_opponent_agent       34.884615\n",
       "13          statistical_agent       16.038462\n",
       "6              agent_sequence       14.423077\n",
       "4         copy_opponent_agent        3.423077\n",
       "3                random_agent       -0.076923\n",
       "8          reward_check_agent      -15.115385\n",
       "7      common_selection_agent      -30.192308\n",
       "2              scissors_agent      -60.230769\n",
       "1                 paper_agent      -67.923077\n",
       "0                  rock_agent      -82.576923"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agents = [\n",
    "    rock_agent,\n",
    "    paper_agent,\n",
    "    scissors_agent,\n",
    "    random_agent,\n",
    "    copy_opponent_agent,\n",
    "    respond_to_opponent_agent,\n",
    "    agent_sequence,\n",
    "    common_selection_agent,\n",
    "    reward_check_agent,\n",
    "    anti_replay_agent,\n",
    "    predictive_agent,\n",
    "    reactionary_agent,\n",
    "    counter_reactionary_agent,\n",
    "    statistical_agent,\n",
    "]\n",
    "print(f\"Всего агентов: {len(agents)}\")\n",
    "results = {agent.__name__: 0 for agent in agents}\n",
    "match_results = []\n",
    "\n",
    "for i in range(len(agents)):\n",
    "    for j in range(len(agents)):\n",
    "        if i == j:\n",
    "            continue\n",
    "        rewards = evaluate(\n",
    "            \"rps\",\n",
    "            [agents[i], agents[j]],\n",
    "            configuration={\"episodeSteps\": 200},\n",
    "            debug=False,\n",
    "        )[0]\n",
    "\n",
    "        match_results.append(\n",
    "            (agents[i].__name__, agents[j].__name__, rewards[0], rewards[1])\n",
    "        )\n",
    "\n",
    "        if rewards[0] > rewards[1]:\n",
    "            results[agents[i].__name__] += 1\n",
    "        elif rewards[1] > rewards[0]:\n",
    "            results[agents[j].__name__] += 1\n",
    "\n",
    "for agent_name, score in sorted(\n",
    "    results.items(), key=lambda item: item[1], reverse=True\n",
    "):\n",
    "    print(f\"{agent_name}: {score} побед\")\n",
    "\n",
    "df_rewards = pd.DataFrame(\n",
    "    match_results, columns=[\"Agent 1\", \"Agent 2\", \"Rewards Agent 1\", \"Rewards Agent 2\"]\n",
    ")\n",
    "results_list = []\n",
    "\n",
    "for agent in {agent.__name__: 0 for agent in agents}:\n",
    "    rewards_agent1, rewards_agent2 = (\n",
    "        df_rewards[df_rewards[\"Agent 1\"] == agent][\"Rewards Agent 1\"],\n",
    "        df_rewards[df_rewards[\"Agent 2\"] == agent][\"Rewards Agent 2\"],\n",
    "    )\n",
    "    results_list.append(\n",
    "        {\n",
    "            \"Agent\": agent,\n",
    "            \"Average Reward\": pd.concat([rewards_agent1, rewards_agent2]).mean(),\n",
    "        }\n",
    "    )\n",
    "pd.DataFrame(results_list).sort_values(by=\"Average Reward\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Вывод\n",
    "\n",
    "Топ-3 агентов по количеству побед:\n",
    "\n",
    "* counter_reactionary_agent: 15 побед\n",
    "* anti_replay_agent: 13 побед\n",
    "* reactionary_agent: 12 побед\n",
    "\n",
    "Топ-3 агентов по среднему reward:\n",
    "\n",
    "* counter_reactionary_agent\t61.000000\n",
    "* reactionary_agent\t52.653846\n",
    "* anti_replay_agent\t38.307692\n",
    "\n",
    "Тактика, используемая агентом `counter_reactionary_agent`, оказалась наиболее эффективной."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
